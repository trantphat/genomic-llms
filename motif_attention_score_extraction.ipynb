{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import BertAttentionScoreExtractor\n",
    "from utils.data_preprocessing import load_dataset\n",
    "from utils.attention_utils import extract_kmer_attention_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--batch_size\", type=int, default=8, help=\"\") # Set default value to 8 because the size of the dataset (2968) is divisible by 8\n",
    "parser.add_argument(\"--max_length\", type=int, default=200, help=\"\")\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory and file path for saving attention scores\n",
    "output_dir = \"./outputs/attention_scores\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores shape for 3-mer: (2968, 12, 200, 200)\n",
      "3-mer attention scores saved to ./outputs/attention_scores/2025-02-27_V2/3-mer_train_attention_scores.npy\n",
      "Size: (2968, 200)\n",
      "Attention scores shape for 4-mer: (2968, 12, 199, 199)\n",
      "4-mer attention scores saved to ./outputs/attention_scores/2025-02-27_V2/4-mer_train_attention_scores.npy\n",
      "Size: (2968, 200)\n",
      "Attention scores shape for 5-mer: (2968, 12, 198, 198)\n",
      "5-mer attention scores saved to ./outputs/attention_scores/2025-02-27_V2/5-mer_train_attention_scores.npy\n",
      "Size: (2968, 200)\n",
      "Attention scores shape for 6-mer: (2968, 12, 197, 197)\n",
      "6-mer attention scores saved to ./outputs/attention_scores/2025-02-27_V2/6-mer_train_attention_scores.npy\n",
      "Size: (2968, 200)\n"
     ]
    }
   ],
   "source": [
    "kmer_values = [3, 4, 5, 6]\n",
    "model_date = \"2025-02-27_V2\"\n",
    "\n",
    "for kmer in kmer_values:\n",
    "    args.kmer = kmer\n",
    "    args.model_path = f\"./outputs/identifier_models/{model_date}/{kmer}-mer\"\n",
    "    args.train_data_path = f\"./data/enhancer_identification/{kmer}-mer_identification_train.txt\"\n",
    "\n",
    "    model = BertAttentionScoreExtractor.from_pretrained(args.model_path, output_attentions=True).to(device)\n",
    "\n",
    "    train_dataset = load_dataset(args, validation=False)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    train_attention_scores = extract_kmer_attention_vectors(model, train_dataloader, args)\n",
    "\n",
    "    file_path = f\"./outputs/attention_scores/{model_date}/{args.kmer}-mer_train_attention_scores.npy\"\n",
    "    os.makedirs(f\"./outputs/attention_scores/{model_date}\", exist_ok=True)\n",
    "    \n",
    "    # Save the train_attention_scores\n",
    "    np.save(file_path, train_attention_scores)\n",
    "\n",
    "    print(f\"{kmer}-mer attention scores saved to {file_path}\")\n",
    "    print(f\"Size: {train_attention_scores.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
